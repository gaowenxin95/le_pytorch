---
title: pytorch学习笔记
author: 高文欣
date: "`r Sys.Date()`"
output: 
    bookdown::gitbook:
        split_by: none
        split_bib: TRUE
        df_print: paged
bibliography: refs/add.bib
---

# pytorch学习笔记

## 数据的加载和预处理

>PyTorch通过torch.utils.data对一般常用的数据加载进行了封装，可以很容易地实现多线程数据预读和批量加载。

猜测是工程上的实现，应该是更快的吧~


### Dataset类


**自定义数据集**

Dataset是一个抽象类，为了能够方便的读取，需要将要使用的数据包装为Dataset类。自定义的Dataset需要继承它并且实现两个成员方法：

/__getitem__() 该方法定义用索引(0 到 len(self))获取一条数据或一个样本
/__len__() 该方法返回数据集的总长度


```r
from torch.utils.data import Dataset
import pandas as pd
class BulldozerDataset(Dataset):
    """ 数据集演示 """
    def __init__(self, csv_file):
        """实现初始化方法，在初始化的时候将数据读载入"""
        self.df=pd.read_csv(csv_file)
    def __len__(self):
        '''
        返回df的长度
        '''
        return len(self.df)
    def __getitem__(self, idx):
        '''
        根据 idx 返回一行数据
        '''
        return self.df.iloc[idx].SalePrice
```

```r
ds_demo= BulldozerDataset('median_benchmark.csv')
```

```r
#实现了 __len__ 方法所以可以直接使用len获取数据总数
len(ds_demo)
```

```r

#用索引可以直接访问对应的数据，对应 __getitem__ 方法
ds_demo[0]
```

>并且torchvision已经预先实现了常用图像数据集，包括前面使用过的CIFAR-10，ImageNet、COCO、MNIST、LSUN等数据集，可通过torchvision.datasets方便的调用。

### Dataloader

DataLoader为我们提供了对Dataset的读取操作，常用参数有：batch_size(每个batch的大小)、 shuffle(是否进行shuffle操作)、 num_workers(加载数据的时候使用几个子进程)。下面做一个简单的操作

```r
dl = torch.utils.data.DataLoader(ds_demo, batch_size=10, shuffle=True, num_workers=0)
```

DataLoader返回的是一个可迭代对象，我们可以使用迭代器分次获取数据

```r
idata=iter(dl)
print(next(idata))
tensor([24000., 24000., 24000., 24000., 24000., 24000., 24000., 24000., 24000.,
        24000.], dtype=torch.float64)
        
```

常见的用法是使用for循环对其进行遍历

```r
for i, data in enumerate(dl):
    print(i,data)
    # 为了节约空间，这里只循环一遍
    break
    
# 0 tensor([24000., 24000., 24000., 24000., 24000., 24000., 24000., 24000., 24000.,
        24000.], dtype=torch.float64)
```

        
使用Datalorder载入和遍历数据集




## torchvision

这个包是专门用来处理图像的

torchvision.datasets 可以理解为PyTorch团队自定义的dataset，这些dataset帮我们提前处理好了很多的图片数据集，我们拿来就可以直接使用：

MNIST
COCO
Captions
Detection
LSUN
ImageFolder
Imagenet-12
CIFAR
STL10
SVHN
PhotoTour 我们可以直接使用，示例如下：
```r

import torchvision.datasets as datasets
trainset = datasets.MNIST(root='./data', # 表示 MNIST 数据的加载的目录
                                      train=True,  # 表示是否加载数据库的训练集，false的时候加载测试集
                                      download=True, # 表示是否自动下载 MNIST 数据集
                                      transform=None) # 表示是否需要对数据进行预处理，none为不进行预处理
```


### torchvision.models

torchvision不仅提供了常用图片数据集，还提供了训练好的模型，可以加载之后，直接使用，或者在进行迁移学习 torchvision.models模块的 子模块中包含以下模型结构。

- AlexNet
- VGG
- ResNet
- SqueezeNet
- DenseNet

```r
#我们直接可以使用训练好的模型，当然这个与datasets相同，都是需要从服务器下载的
import torchvision.models as models
resnet18 = models.resnet18(pretrained=True)
```

### 

torchvision.transforms
transforms 模块提供了一般的图像转换操作类，用作数据处理和数据增强

```r
from torchvision import transforms as transforms
transform = transforms.Compose([
    transforms.RandomCrop(32, padding=4),  #先四周填充0，在把图像随机裁剪成32*32
    transforms.RandomHorizontalFlip(),  #图像一半的概率翻转，一半的概率不翻转
    transforms.RandomRotation((-45,45)), #随机旋转
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.229, 0.224, 0.225)), #R,G,B每层的归一化用到的均值和方差
])

```

张量的各种操作就是类似于numpy的各种操作

具体的张量的操作可以参考[github张量](https://github.com/zergtant/pytorch-handbook/blob/master/chapter1/1_tensor_tutorial.ipynb)

**torch.view 与Numpy的reshape类似**


## 定义一个卷积神经网络


从之前的神经网络一节复制神经网络代码，并修改为输入3通道图像。

```r
import torch.nn as nn
import torch.nn.functional as F


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x


net = Net()

```

## 定义损失函数和优化器

我们使用交叉熵作为损失函数，使用带动量的随机梯度下降。

```r
import torch.optim as optim

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
```

## 4. 训练网路

有趣的时刻开始了。 我们只需在数据迭代器上循环，将数据输入给网络，并优化。

```r
for epoch in range(2):  # 多批次循环

    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        # 获取输入
        inputs, labels = data

        # 梯度置0
        optimizer.zero_grad()

        # 正向传播，反向传播，优化
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # 打印状态信息
        running_loss += loss.item()
        if i % 2000 == 1999:    # 每2000批次打印一次
            print('[%d, %5d] loss: %.3f' %
                  (epoch + 1, i + 1, running_loss / 2000))
            running_loss = 0.0

print('Finished Training')

```


